{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from nltk import pos_tag, word_tokenize\n",
    "import re\n",
    "\n",
    "import time\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn import feature_extraction, linear_model, model_selection, preprocessing\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.naive_bayes import BernoulliNB\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.gaussian_process import GaussianProcessClassifier\n",
    "from keras.models import Model\n",
    "from keras.layers import LSTM, Activation, Dense, Dropout, Input, Embedding\n",
    "from keras.optimizers import RMSprop\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing import sequence\n",
    "from keras.callbacks import EarlyStopping\n",
    "\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix,classification_report\n",
    "\n",
    "import nltk\n",
    "import nltk as nlp\n",
    "import string\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv('Datasets\\Single Source\\Treated\\combined_train_treated.csv')\n",
    "test = pd.read_csv('Datasets\\Single Source\\Treated\\combined_test_treated.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['text'].fillna(\"No Text\", inplace = True)\n",
    "test['text'].fillna(\"No Text\", inplace = True)\n",
    "\n",
    "train_copy = train.copy(deep = True)\n",
    "test_copy = test.copy(deep = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy: 95.55%\n",
      "CONFUSION MATRIX\n",
      "[[3276  205]\n",
      " [  13 1403]]\n"
     ]
    }
   ],
   "source": [
    "pipe = Pipeline([('vect', CountVectorizer()),\n",
    "                 ('tfidf', TfidfTransformer()),\n",
    "                 ('model', LogisticRegression())])\n",
    "\n",
    "model = pipe.fit(train_copy['text'],train_copy['target'])\n",
    "pred = model.predict(test_copy['text'])\n",
    "\n",
    "print(\"accuracy: {}%\".format(round(accuracy_score(test_copy['target'],\n",
    "                                                  pred)*100,2)))\n",
    "\n",
    "print(\"CONFUSION MATRIX\")\n",
    "print(confusion_matrix(test_copy['target'],\n",
    "                      pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Support Vector Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy: 96.98%\n",
      "CONFUSION MATRIX\n",
      "[[3345  136]\n",
      " [  12 1404]]\n"
     ]
    }
   ],
   "source": [
    "pipe = Pipeline([('vect', CountVectorizer()),\n",
    "                 ('tfidf', TfidfTransformer()),\n",
    "                 ('model', LinearSVC())])\n",
    "\n",
    "model = pipe.fit(train_copy['text'],train_copy['target'])\n",
    "pred = model.predict(test_copy['text'])\n",
    "\n",
    "print(\"accuracy: {}%\".format(round(accuracy_score(test_copy['target'],\n",
    "                                                  pred)*100,2)))\n",
    "\n",
    "print(\"CONFUSION MATRIX\")\n",
    "print(confusion_matrix(test_copy['target'],\n",
    "                      pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multinomial Naive Bayes Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy: 96.98%\n",
      "CONFUSION MATRIX\n",
      "[[3345  136]\n",
      " [  12 1404]]\n"
     ]
    }
   ],
   "source": [
    "pipe = Pipeline([('vect', CountVectorizer()),\n",
    "                 ('tfidf', TfidfTransformer()),\n",
    "                 ('model', LinearSVC())])\n",
    "\n",
    "model = pipe.fit(train_copy['text'],train_copy['target'])\n",
    "pred = model.predict(test_copy['text'])\n",
    "\n",
    "print(\"accuracy: {}%\".format(round(accuracy_score(test_copy['target'],\n",
    "                                                  pred)*100,2)))\n",
    "\n",
    "print(\"CONFUSION MATRIX\")\n",
    "print(confusion_matrix(test_copy['target'],\n",
    "                      pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient Boost Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy: 80.09%\n",
      "CONFUSION MATRIX\n",
      "[[2637  844]\n",
      " [ 131 1285]]\n"
     ]
    }
   ],
   "source": [
    "pipe = Pipeline([('vect', CountVectorizer()),\n",
    "                 ('tfidf', TfidfTransformer()),\n",
    "                 ('model', GradientBoostingClassifier(loss = 'deviance',\n",
    "                                                   learning_rate = 0.01,\n",
    "                                                   n_estimators = 10,\n",
    "                                                   max_depth = 5,\n",
    "                                                   random_state=42))])\n",
    "\n",
    "model = pipe.fit(train_copy['text'],train_copy['target'])\n",
    "pred = model.predict(test_copy['text'])\n",
    "\n",
    "print(\"accuracy: {}%\".format(round(accuracy_score(test_copy['target'],\n",
    "                                                  pred)*100,2)))\n",
    "print(\"CONFUSION MATRIX\")\n",
    "print(confusion_matrix(test_copy['target'],\n",
    "                      pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## XGBoost Classifier*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy: 80.15%\n",
      "CONFUSION MATRIX\n",
      "[[2641  840]\n",
      " [ 132 1284]]\n"
     ]
    }
   ],
   "source": [
    "pipe = Pipeline([('vect', CountVectorizer()),\n",
    "                 ('tfidf', TfidfTransformer()),\n",
    "                 ('model', XGBClassifier(loss = 'deviance',\n",
    "                                        learning_rate = 0.01,\n",
    "                                        n_estimators = 10,\n",
    "                                        max_depth = 5,\n",
    "                                        random_state=2020))])\n",
    "\n",
    "model = pipe.fit(train_copy['text'],train_copy['target'])\n",
    "pred = model.predict(test_copy['text'])\n",
    "\n",
    "print(\"accuracy: {}%\".format(round(accuracy_score(test_copy['target'],\n",
    "                                                  pred)*100,2)))\n",
    "print(\"CONFUSION MATRIX\")\n",
    "print(confusion_matrix(test_copy['target'],\n",
    "                      pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forest Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy: 92.14%\n",
      "CONFUSION MATRIX\n",
      "[[3115  366]\n",
      " [  19 1397]]\n"
     ]
    }
   ],
   "source": [
    "pipe = Pipeline([('vect', CountVectorizer()),\n",
    "                 ('tfidf', TfidfTransformer()),\n",
    "                 ('model', RandomForestClassifier())])\n",
    "\n",
    "model = pipe.fit(train_copy['text'],train_copy['target'])\n",
    "pred = model.predict(test_copy['text'])\n",
    "\n",
    "print(\"accuracy: {}%\".format(round(accuracy_score(test_copy['target'],\n",
    "                                                  pred)*100,2)))\n",
    "\n",
    "print(\"CONFUSION MATRIX\")\n",
    "print(confusion_matrix(test_copy['target'],\n",
    "                      pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LSTM - RNN Baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = train_copy['text']\n",
    "Y = train_copy['target']\n",
    "\n",
    "le = LabelEncoder()\n",
    "Y = le.fit_transform(Y)\n",
    "Y = Y.reshape(-1,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_words = 500\n",
    "max_len = 1000\n",
    "\n",
    "tok = Tokenizer(num_words = max_words)\n",
    "tok.fit_on_texts(X)\n",
    "\n",
    "sequences = tok.texts_to_sequences(X)\n",
    "sequences_matrix = sequence.pad_sequences(sequences, maxlen = max_len)\n",
    "\n",
    "def RNN():\n",
    "    inputs = Input(name='inputs',shape=[max_len])\n",
    "    layer = Embedding(max_words,50,input_length=max_len)(inputs)\n",
    "    layer = LSTM(64)(layer)\n",
    "    layer = Dense(256,name='FC1')(layer)\n",
    "    layer = Activation('relu')(layer)\n",
    "    layer = Dropout(0.5)(layer)\n",
    "    layer = Dense(1,name='out_layer')(layer)\n",
    "    layer = Activation('sigmoid')(layer)\n",
    "    model = Model(inputs=inputs,outputs=layer)\n",
    "    return model\n",
    "\n",
    "model = RNN()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss = 'binary_crossentropy', optimizer = RMSprop(),\n",
    "             metrics = ['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "inputs (InputLayer)          (None, 1000)              0         \n",
      "_________________________________________________________________\n",
      "embedding_1 (Embedding)      (None, 1000, 50)          25000     \n",
      "_________________________________________________________________\n",
      "lstm_1 (LSTM)                (None, 64)                29440     \n",
      "_________________________________________________________________\n",
      "FC1 (Dense)                  (None, 256)               16640     \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "out_layer (Dense)            (None, 1)                 257       \n",
      "_________________________________________________________________\n",
      "activation_2 (Activation)    (None, 1)                 0         \n",
      "=================================================================\n",
      "Total params: 71,337\n",
      "Trainable params: 71,337\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Siraz\\Anaconda3\\envs\\gpu\\lib\\site-packages\\tensorflow_core\\python\\framework\\indexed_slices.py:433: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 32000 samples, validate on 8000 samples\n",
      "Epoch 1/20\n",
      "32000/32000 [==============================] - 101s 3ms/step - loss: 0.2460 - accuracy: 0.9121 - val_loss: 0.1266 - val_accuracy: 0.9586\n",
      "Epoch 2/20\n",
      "32000/32000 [==============================] - 123s 4ms/step - loss: 0.1233 - accuracy: 0.9592 - val_loss: 0.0975 - val_accuracy: 0.9682\n",
      "Epoch 3/20\n",
      "32000/32000 [==============================] - 142s 4ms/step - loss: 0.1041 - accuracy: 0.9668 - val_loss: 0.0908 - val_accuracy: 0.9711\n",
      "Epoch 4/20\n",
      "32000/32000 [==============================] - 193s 6ms/step - loss: 0.0872 - accuracy: 0.9728 - val_loss: 0.0898 - val_accuracy: 0.9726\n",
      "Epoch 5/20\n",
      "32000/32000 [==============================] - 125s 4ms/step - loss: 0.0744 - accuracy: 0.9762 - val_loss: 0.0752 - val_accuracy: 0.9783\n",
      "Epoch 6/20\n",
      "32000/32000 [==============================] - 118s 4ms/step - loss: 0.0659 - accuracy: 0.9787 - val_loss: 0.0638 - val_accuracy: 0.9790\n",
      "Epoch 7/20\n",
      "32000/32000 [==============================] - 112s 4ms/step - loss: 0.0575 - accuracy: 0.9814 - val_loss: 0.0650 - val_accuracy: 0.9810\n",
      "Epoch 8/20\n",
      "32000/32000 [==============================] - 99s 3ms/step - loss: 0.0541 - accuracy: 0.9826 - val_loss: 0.0614 - val_accuracy: 0.9800\n",
      "Epoch 9/20\n",
      "32000/32000 [==============================] - 95s 3ms/step - loss: 0.0492 - accuracy: 0.9839 - val_loss: 0.0634 - val_accuracy: 0.9811\n",
      "Epoch 10/20\n",
      "32000/32000 [==============================] - 96s 3ms/step - loss: 0.0461 - accuracy: 0.9855 - val_loss: 0.0626 - val_accuracy: 0.9809\n",
      "Epoch 11/20\n",
      "32000/32000 [==============================] - 95s 3ms/step - loss: 0.0413 - accuracy: 0.9866 - val_loss: 0.1085 - val_accuracy: 0.9669\n",
      "Epoch 12/20\n",
      "32000/32000 [==============================] - 95s 3ms/step - loss: 0.0402 - accuracy: 0.9875 - val_loss: 0.0675 - val_accuracy: 0.9799\n",
      "Epoch 13/20\n",
      "32000/32000 [==============================] - 95s 3ms/step - loss: 0.0347 - accuracy: 0.9888 - val_loss: 0.0757 - val_accuracy: 0.9758\n",
      "Epoch 14/20\n",
      "32000/32000 [==============================] - 95s 3ms/step - loss: 0.0331 - accuracy: 0.9902 - val_loss: 0.0914 - val_accuracy: 0.9785\n",
      "Epoch 15/20\n",
      "32000/32000 [==============================] - 96s 3ms/step - loss: 0.0311 - accuracy: 0.9908 - val_loss: 0.0764 - val_accuracy: 0.9789\n",
      "Epoch 16/20\n",
      "32000/32000 [==============================] - 95s 3ms/step - loss: 0.0283 - accuracy: 0.9915 - val_loss: 0.0725 - val_accuracy: 0.9803\n",
      "Epoch 17/20\n",
      "32000/32000 [==============================] - 95s 3ms/step - loss: 0.0270 - accuracy: 0.9922 - val_loss: 0.0727 - val_accuracy: 0.9806\n",
      "Epoch 18/20\n",
      "32000/32000 [==============================] - 262s 8ms/step - loss: 0.0236 - accuracy: 0.9931 - val_loss: 0.0747 - val_accuracy: 0.9799\n",
      "Epoch 19/20\n",
      "32000/32000 [==============================] - 105s 3ms/step - loss: 0.0256 - accuracy: 0.9926 - val_loss: 0.0919 - val_accuracy: 0.9747\n",
      "Epoch 20/20\n",
      "32000/32000 [==============================] - 103s 3ms/step - loss: 0.0200 - accuracy: 0.9940 - val_loss: 0.1107 - val_accuracy: 0.9719\n",
      "Time Taken =  2340.764161348343\n"
     ]
    }
   ],
   "source": [
    "#TAKES TIME. Fitting on GTX 1050ti.\n",
    "t1 = time.time()\n",
    "\n",
    "model.fit(sequences_matrix, Y, batch_size = 256, epochs = 20,\n",
    "         validation_split = 0.2)\n",
    "\n",
    "t2 = time.time()\n",
    "print(\"Time Taken = \", t2-t1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LSTM on test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4897/4897 [==============================] - 20s 4ms/step\n",
      "Accuracy :0.88\n"
     ]
    }
   ],
   "source": [
    "test_sequences = tok.texts_to_sequences(test_copy['text'])\n",
    "test_sequences_matrix = sequence.pad_sequences(test_sequences, \n",
    "                                               maxlen = max_len)\n",
    "\n",
    "accr = model.evaluate(test_sequences_matrix, test_copy['target'])\n",
    "print('Accuracy :{:0.2f}'.format(accr[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "#SAVING MODEL\n",
    "\n",
    "# serialize model to JSON\n",
    "model_json = model.to_json()\n",
    "with open(\"ideal_LSTM_arc.json\", \"w\") as json_file:\n",
    "    json_file.write(model_json)\n",
    "# serialize weights to HDF5\n",
    "model.save_weights(\"ideal_LSTM_weights.h5\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.7 64-bit ('gpu': conda)",
   "language": "python",
   "name": "python37764bitgpuconda08766899e99640f8bf22a5cda132f1fd"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
