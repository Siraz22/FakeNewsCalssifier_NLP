{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from nltk import pos_tag, word_tokenize\n",
    "import re\n",
    "\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn import feature_extraction, linear_model, model_selection, preprocessing\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.naive_bayes import BernoulliNB\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.gaussian_process import GaussianProcessClassifier\n",
    "from keras.models import Model\n",
    "from keras.layers import LSTM, Activation, Dense, Dropout, Input, Embedding\n",
    "from keras.optimizers import RMSprop\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing import sequence\n",
    "from keras.callbacks import EarlyStopping\n",
    "\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix,classification_report\n",
    "\n",
    "import nltk\n",
    "import nltk as nlp\n",
    "import string\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv('no_stopwords_combined.csv')\n",
    "test = pd.read_csv('testnostopwords.csv')\n",
    "\n",
    "train_copy = train.copy(deep = True)\n",
    "test_copy = test.copy(deep = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>Unnamed: 0.1</th>\n",
       "      <th>title</th>\n",
       "      <th>text</th>\n",
       "      <th>subject</th>\n",
       "      <th>date</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Ramaphosa's ANC election win lifts South Afric...</td>\n",
       "      <td>johannesburg reuters south african banking sto...</td>\n",
       "      <td>worldnews</td>\n",
       "      <td>December 19, 2017</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>VIGILANTE PIRATES INTERCEDE Where Government F...</td>\n",
       "      <td>like soldiers oden vigilante group reported fe...</td>\n",
       "      <td>Government News</td>\n",
       "      <td>Apr 1, 2016</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>SICK! DEMOCRAT ORGANIZER, Mayor DeBlasio Emplo...</td>\n",
       "      <td>last week huma abedin husband anthony weiner w...</td>\n",
       "      <td>left-news</td>\n",
       "      <td>May 28, 2017</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0  Unnamed: 0.1  \\\n",
       "0           0             0   \n",
       "1           1             1   \n",
       "2           2             2   \n",
       "\n",
       "                                               title  \\\n",
       "0  Ramaphosa's ANC election win lifts South Afric...   \n",
       "1  VIGILANTE PIRATES INTERCEDE Where Government F...   \n",
       "2  SICK! DEMOCRAT ORGANIZER, Mayor DeBlasio Emplo...   \n",
       "\n",
       "                                                text          subject  \\\n",
       "0  johannesburg reuters south african banking sto...        worldnews   \n",
       "1  like soldiers oden vigilante group reported fe...  Government News   \n",
       "2  last week huma abedin husband anthony weiner w...        left-news   \n",
       "\n",
       "                 date  target  \n",
       "0  December 19, 2017        1  \n",
       "1         Apr 1, 2016       0  \n",
       "2        May 28, 2017       0  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_copy.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Unnamed: 0        0\n",
       "Unnamed: 0.1      0\n",
       "title             0\n",
       "text            718\n",
       "subject           0\n",
       "date              0\n",
       "target            0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_copy.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Starter Code EDA Result\n",
    "\n",
    "- It is better to use the NaN rows, as all of them correspond to fake news\n",
    "- One of the local rows in the dataset with null text is however labelled \"true news\"\n",
    "- Pipeline doesn;t work with NaN values, so we'll replace NaN by 'Empty' string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train_copy.dropna(subset = ['text'], inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_copy.fillna('Empty', inplace = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Reg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy: 52.49%\n"
     ]
    }
   ],
   "source": [
    "pipe = Pipeline([('vect', CountVectorizer()),\n",
    "                 ('tfidf', TfidfTransformer()),\n",
    "                 ('model', LogisticRegression())])\n",
    "\n",
    "model = pipe.fit(train_copy['text'],train_copy['target'])\n",
    "pred = model.predict(test_copy['text'])\n",
    "\n",
    "print(\"accuracy: {}%\".format(round(accuracy_score(test_copy['target'],\n",
    "                                                  pred)*100,2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[131 247]\n",
      " [135 291]]\n"
     ]
    }
   ],
   "source": [
    "print(confusion_matrix(test_copy['target'],\n",
    "                      pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Support Vector Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy: 50.62%\n"
     ]
    }
   ],
   "source": [
    "pipe = Pipeline([('vect', CountVectorizer()),\n",
    "                 ('tfidf', TfidfTransformer()),\n",
    "                 ('model', LinearSVC())])\n",
    "\n",
    "model = pipe.fit(train_copy['text'],train_copy['target'])\n",
    "pred = model.predict(test_copy['text'])\n",
    "\n",
    "print(\"accuracy: {}%\".format(round(accuracy_score(test_copy['target'],\n",
    "                                                  pred)*100,2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[155 223]\n",
      " [174 252]]\n"
     ]
    }
   ],
   "source": [
    "print(confusion_matrix(test_copy['target'],\n",
    "                      pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multinomial Naive Bayes Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy: 50.62%\n"
     ]
    }
   ],
   "source": [
    "pipe = Pipeline([('vect', CountVectorizer()),\n",
    "                 ('tfidf', TfidfTransformer()),\n",
    "                 ('model', LinearSVC())])\n",
    "\n",
    "model = pipe.fit(train_copy['text'],train_copy['target'])\n",
    "pred = model.predict(test_copy['text'])\n",
    "\n",
    "print(\"accuracy: {}%\".format(round(accuracy_score(test_copy['target'],\n",
    "                                                  pred)*100,2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[155 223]\n",
      " [174 252]]\n"
     ]
    }
   ],
   "source": [
    "print(confusion_matrix(test_copy['target'],\n",
    "                      pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient Boost Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy: 48.51%\n",
      "[[330  48]\n",
      " [366  60]]\n"
     ]
    }
   ],
   "source": [
    "pipe = Pipeline([('vect', CountVectorizer()),\n",
    "                 ('tfidf', TfidfTransformer()),\n",
    "                 ('model', GradientBoostingClassifier(loss = 'deviance',\n",
    "                                                   learning_rate = 0.01,\n",
    "                                                   n_estimators = 10,\n",
    "                                                   max_depth = 5,\n",
    "                                                   random_state=42))])\n",
    "\n",
    "model = pipe.fit(train_copy['text'],train_copy['target'])\n",
    "pred = model.predict(test_copy['text'])\n",
    "\n",
    "print(\"accuracy: {}%\".format(round(accuracy_score(test_copy['target'],\n",
    "                                                  pred)*100,2)))\n",
    "\n",
    "print(confusion_matrix(test_copy['target'],\n",
    "                      pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## XGBoost Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy: 48.51%\n",
      "[[330  48]\n",
      " [366  60]]\n"
     ]
    }
   ],
   "source": [
    "pipe = Pipeline([('vect', CountVectorizer()),\n",
    "                 ('tfidf', TfidfTransformer()),\n",
    "                 ('model', XGBClassifier(loss = 'deviance',\n",
    "                                        learning_rate = 0.01,\n",
    "                                        n_estimators = 10,\n",
    "                                        max_depth = 5,\n",
    "                                        random_state=2020))])\n",
    "\n",
    "model = pipe.fit(train_copy['text'],train_copy['target'])\n",
    "pred = model.predict(test_copy['text'])\n",
    "\n",
    "print(\"accuracy: {}%\".format(round(accuracy_score(test_copy['target'],\n",
    "                                                  pred)*100,2)))\n",
    "\n",
    "print(confusion_matrix(test_copy['target'],\n",
    "                      pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forest Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy: 52.61%\n",
      "[[ 28 350]\n",
      " [ 31 395]]\n"
     ]
    }
   ],
   "source": [
    "pipe = Pipeline([('vect', CountVectorizer()),\n",
    "                 ('tfidf', TfidfTransformer()),\n",
    "                 ('model', RandomForestClassifier())])\n",
    "\n",
    "model = pipe.fit(train_copy['text'],train_copy['target'])\n",
    "pred = model.predict(test_copy['text'])\n",
    "\n",
    "print(\"accuracy: {}%\".format(round(accuracy_score(test_copy['target'],\n",
    "                                                  pred)*100,2)))\n",
    "\n",
    "print(confusion_matrix(test_copy['target'],\n",
    "                      pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LSTM - Recurrent Neural Nets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = train_copy['text']\n",
    "Y = train_copy['target']\n",
    "\n",
    "le = LabelEncoder()\n",
    "Y = le.fit_transform(Y)\n",
    "Y = Y.reshape(-1,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_words = 500\n",
    "max_len = 1000\n",
    "\n",
    "tok = Tokenizer(num_words = max_words)\n",
    "tok.fit_on_texts(X)\n",
    "\n",
    "sequences = tok.texts_to_sequences(X)\n",
    "sequences_matrix = sequence.pad_sequences(sequences, maxlen = max_len)\n",
    "\n",
    "def RNN():\n",
    "    inputs = Input(name='inputs',shape=[max_len])\n",
    "    layer = Embedding(max_words,50,input_length=max_len)(inputs)\n",
    "    layer = LSTM(64)(layer)\n",
    "    layer = Dense(256,name='FC1')(layer)\n",
    "    layer = Activation('relu')(layer)\n",
    "    layer = Dropout(0.5)(layer)\n",
    "    layer = Dense(1,name='out_layer')(layer)\n",
    "    layer = Activation('sigmoid')(layer)\n",
    "    model = Model(inputs=inputs,outputs=layer)\n",
    "    return model\n",
    "\n",
    "model = RNN()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss = 'binary_crossentropy', optimizer = RMSprop(),\n",
    "             metrics = ['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "inputs (InputLayer)          (None, 1000)              0         \n",
      "_________________________________________________________________\n",
      "embedding_1 (Embedding)      (None, 1000, 50)          25000     \n",
      "_________________________________________________________________\n",
      "lstm_1 (LSTM)                (None, 64)                29440     \n",
      "_________________________________________________________________\n",
      "FC1 (Dense)                  (None, 256)               16640     \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "out_layer (Dense)            (None, 1)                 257       \n",
      "_________________________________________________________________\n",
      "activation_2 (Activation)    (None, 1)                 0         \n",
      "=================================================================\n",
      "Total params: 71,337\n",
      "Trainable params: 71,337\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Siraz\\Anaconda3\\envs\\gpu\\lib\\site-packages\\tensorflow_core\\python\\framework\\indexed_slices.py:433: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 35918 samples, validate on 8980 samples\n",
      "Epoch 1/20\n",
      "35918/35918 [==============================] - 214s 6ms/step - loss: 0.2201 - accuracy: 0.9211 - val_loss: 0.1218 - val_accuracy: 0.9596\n",
      "Epoch 2/20\n",
      "35918/35918 [==============================] - 210s 6ms/step - loss: 0.0701 - accuracy: 0.9790 - val_loss: 0.0186 - val_accuracy: 0.9961\n",
      "Epoch 3/20\n",
      "35918/35918 [==============================] - 211s 6ms/step - loss: 0.0297 - accuracy: 0.9928 - val_loss: 0.0116 - val_accuracy: 0.9979\n",
      "Epoch 4/20\n",
      "35918/35918 [==============================] - 216s 6ms/step - loss: 0.0413 - accuracy: 0.9890 - val_loss: 0.0224 - val_accuracy: 0.9967\n",
      "Epoch 5/20\n",
      "35918/35918 [==============================] - 209s 6ms/step - loss: 0.0326 - accuracy: 0.9912 - val_loss: 0.0620 - val_accuracy: 0.9976\n",
      "Epoch 6/20\n",
      "35918/35918 [==============================] - 244s 7ms/step - loss: 0.0378 - accuracy: 0.9911 - val_loss: 0.0108 - val_accuracy: 0.9976\n",
      "Epoch 7/20\n",
      "35918/35918 [==============================] - 266s 7ms/step - loss: 0.0387 - accuracy: 0.9845 - val_loss: 0.0364 - val_accuracy: 0.9846\n",
      "Epoch 8/20\n",
      "35918/35918 [==============================] - 237s 7ms/step - loss: 0.0689 - accuracy: 0.9801 - val_loss: 0.0141 - val_accuracy: 0.9974\n",
      "Epoch 9/20\n",
      "35918/35918 [==============================] - 225s 6ms/step - loss: 0.0669 - accuracy: 0.9774 - val_loss: 0.0171 - val_accuracy: 0.9970\n",
      "Epoch 10/20\n",
      "35918/35918 [==============================] - 208s 6ms/step - loss: 0.0670 - accuracy: 0.9788 - val_loss: 0.0153 - val_accuracy: 0.9968\n",
      "Epoch 11/20\n",
      "35918/35918 [==============================] - 227s 6ms/step - loss: 0.0163 - accuracy: 0.9959 - val_loss: 0.0097 - val_accuracy: 0.9977\n",
      "Epoch 12/20\n",
      "35918/35918 [==============================] - 235s 7ms/step - loss: 0.0111 - accuracy: 0.9973 - val_loss: 0.0090 - val_accuracy: 0.9980\n",
      "Epoch 13/20\n",
      "35918/35918 [==============================] - 246s 7ms/step - loss: 0.0096 - accuracy: 0.9975 - val_loss: 0.0147 - val_accuracy: 0.9965\n",
      "Epoch 14/20\n",
      "35918/35918 [==============================] - 238s 7ms/step - loss: 0.0084 - accuracy: 0.9977 - val_loss: 0.0574 - val_accuracy: 0.9853\n",
      "Epoch 15/20\n",
      "35918/35918 [==============================] - 494s 14ms/step - loss: 0.0096 - accuracy: 0.9979 - val_loss: 0.0090 - val_accuracy: 0.9981\n",
      "Epoch 16/20\n",
      "35918/35918 [==============================] - 401s 11ms/step - loss: 0.0067 - accuracy: 0.9985 - val_loss: 0.0392 - val_accuracy: 0.9912\n",
      "Epoch 17/20\n",
      "35918/35918 [==============================] - 273s 8ms/step - loss: 0.0093 - accuracy: 0.9982 - val_loss: 0.0066 - val_accuracy: 0.9987\n",
      "Epoch 18/20\n",
      "35918/35918 [==============================] - 307s 9ms/step - loss: 0.0067 - accuracy: 0.9985 - val_loss: 0.0174 - val_accuracy: 0.9962\n",
      "Epoch 19/20\n",
      "35918/35918 [==============================] - 266s 7ms/step - loss: 0.0041 - accuracy: 0.9992 - val_loss: 0.0323 - val_accuracy: 0.9925\n",
      "Epoch 20/20\n",
      "35918/35918 [==============================] - 352s 10ms/step - loss: 0.0064 - accuracy: 0.9984 - val_loss: 0.0088 - val_accuracy: 0.9982\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.callbacks.History at 0x17db77867c8>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(sequences_matrix, Y, batch_size = 256, epochs = 20,\n",
    "         validation_split = 0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "804/804 [==============================] - 10s 13ms/step\n",
      "Accuracy :0.48\n"
     ]
    }
   ],
   "source": [
    "test_sequences = tok.texts_to_sequences(test_copy['text'])\n",
    "test_sequences_matrix = sequence.pad_sequences(test_sequences, \n",
    "                                               maxlen = max_len)\n",
    "\n",
    "accr = model.evaluate(test_sequences_matrix, test_copy['target'])\n",
    "print('Accuracy :{:0.2f}'.format(accr[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# serialize model to JSON\n",
    "model_json = model.to_json()\n",
    "with open(\"base_model_LSTM_arc.json\", \"w\") as json_file:\n",
    "    json_file.write(model_json)\n",
    "# serialize weights to HDF5\n",
    "model.save_weights(\"base_model_LSTM_weights.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.7 64-bit ('gpu': conda)",
   "language": "python",
   "name": "python37764bitgpuconda08766899e99640f8bf22a5cda132f1fd"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
